import os
from pathlib import Path
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv

from rag import retrieve_context
from prompt import SYSTEM_PROMPT
from memory import add_to_memory, format_memory, clear_memory

<<<<<<< HEAD
load_dotenv()
=======
# â”€â”€ Load environment variables from the .env file next to this script â”€â”€
load_dotenv(dotenv_path=Path(__file__).parent / ".env", override=True)

# â”€â”€ Validate the API key is present and not a placeholder â”€â”€
_api_key = os.getenv("GROQ_KEY")
if not _api_key or _api_key.strip() in ("", "YOUR API KEY", "your_groq_api_key_here"):
    raise EnvironmentError(
        "GROQ_KEY is not set or is still the placeholder value. "
        "Please add your Groq API key to the .env file:\n"
        "  GROQ_KEY=your_actual_key_here"
    )

# â”€â”€ Configuration â”€â”€
GROQ_API_KEY = os.getenv("GROQ_KEY")
if not GROQ_API_KEY:
    st.error("âŒ GROQ_KEY not found in .env file. Please configure your API key.")
    st.stop()
>>>>>>> upstream/main

# Fixed base_url to v1 and model name to instant
client = OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=GROQ_API_KEY
)

TEMPERATURE = float(os.getenv("TEMPERATURE", 0.8))
MAX_TOKENS = int(os.getenv("MAX_TOKENS", 512))
MODEL_NAME = os.getenv("MODEL_NAME", "llama-3.1-8b-instant")

def chat(user_input: str) -> str:
<<<<<<< HEAD
=======
    """Generate a roast response for the user's input using structured messages."""

    # used .strip to remove whitespaces 
>>>>>>> upstream/main
    if not user_input or user_input.isspace():
        return "You sent me nothing? Even your messages are empty, just like your GitHub graph. ğŸ”¥"

<<<<<<< HEAD
    context = retrieve_context(user_input)
    history = format_memory()

    prompt = (
        f"{SYSTEM_PROMPT}\n\n"
        f"Use this roast context for inspiration: {context}\n\n"
        f"Recent conversation for context: {history}"
    )

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": user_input},
        ],
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )

    reply = response.choices[0].message.content
    add_to_memory(user_input, reply)
    return reply
=======
    try:
        # Retrieve relevant roast context via RAG
        context = retrieve_context(user_input)

        # Get conversation history
        history = format_memory()

        # Build structured messages to avoid prompt injection and instruction mixing
        messages = [
            {
                "role": "user",
                "content": (
                    f"Roast context (from knowledge base):\n{context}\n\n"
                    f"Recent conversation:\n{history}\n\n"
                    f"Current message: {user_input}"
                ),
            },
        ]

        # Generate response from Groq using structured system prompt
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                *messages,
            ],
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
        )

        reply = response.choices[0].message.content

        # Store in memory
        add_to_memory(user_input, reply)

        return reply

    except Exception as e:
        st.error(f"Error generating roast: {e}")
        return f"Even I broke trying to roast you. Error: {str(e)[:100]}"
>>>>>>> upstream/main

st.set_page_config(page_title="Super RoastBot", page_icon="ğŸ”¥", layout="centered")
st.title("ğŸ”¥Super RoastBot")
st.caption("I roast harder than your code roasts your CPU")

with st.sidebar:
    st.header("âš™ï¸ Controls")
    if st.button("ğŸ—‘ï¸ Clear Chat"):
        st.session_state.messages = []
        clear_memory()
        st.success("Chat cleared!")
        st.rerun()
<<<<<<< HEAD
=======
    st.divider()
    st.markdown(
        "**How it works:**\n"
        "1. Your message is sent to RAG retrieval\n"
        "2. Relevant roast knowledge is fetched\n"
        "3. Groq crafts a personalized roast\n"
        "4. You cry. Repeat."
    )
    st.divider()
    st.markdown(
        "**âš™ï¸ Config (env-based):**\n"
        f"- Model: `{MODEL_NAME}`\n"
        f"- Temp: `{TEMPERATURE}`\n"
        f"- Max tokens: `{MAX_TOKENS}`"
    )
>>>>>>> upstream/main

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"], avatar="ğŸ˜ˆ" if msg["role"] == "assistant" else "ğŸ¤¡"):
        st.markdown(msg["content"])

if user_input := st.chat_input("Say something... if you dare ğŸ”¥"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user", avatar="ğŸ¤¡"):
        st.markdown(user_input)

    with st.chat_message("assistant", avatar="ğŸ˜ˆ"):
        with st.spinner("Cooking up a roast... ğŸ³"):
<<<<<<< HEAD
            try:
                reply = chat(user_input)
                st.markdown(reply)
                st.session_state.messages.append({"role": "assistant", "content": reply})
            except Exception as e:
                st.error(f"Even I broke trying to roast you. Error: {e}")
=======
            reply = chat(user_input)
            st.markdown(reply)

    st.session_state.messages.append({"role": "assistant", "content": reply})
>>>>>>> upstream/main
